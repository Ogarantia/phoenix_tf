"""
#### Analysis Functions
# low level bench by operation or integrated in a very small NN
 collect_res_by_op/nn:
        collect data in a log file from all python script finishing by "_bench.py" (resp. "_bench_nn.op")
        these tests files should only contain one operation
        to test in the same way small NN, a second function is called to run all python script finishing by "_bench_nn.py"
 generate_graph_by_op/nn:
        Use results in log files generated by "collect_res_by_op" to generate graphs that are saved in a res folder.
        Under the hood, it calls all scripts finishing by "bench_visu.py", which should correspond to one "<op>_bench.py".
        All bench_visu.py should be implement using the same pattern and generate files in the same folder etc.
# High level bench
 generate_tensorboard_train_nn:
        Run all train scripts in tests/python_high_level_tests/; the name must start by "train_"
        All results are sent to {context["resdir"]}/log_<file_name>
 generate_tensorboard_infer_nn:
        Same as previously, but with inference tests; the name must start by "inf_"
        All results are sent to {context["resdir"]}/log_<file_name>
# High high level tests using specific architectures
 generate_imagenet_infer_results:
        Clone imagenet-baseline repository and run inference_benchmark.py,
        in order to get results in {context["resdir"]}
# To run all the functions above
 generate_results:
        Run all above functions

#### Misc Functions
 get_system_info:
    Return a list of information about the system (Architecture and OS); more will be added in the future
 get_available_gpus:
    Return the list of available gpus, empty list if no gpu were found.
 get_project_path:
    Get path to current project. By default look for the name "phoenix_tf"
 get_lib_path:
    Get the path to _upstride.so
 pretty_print:
    Protect your eyes from bleeding reading ugly messages
 fill_context:
    Fill all fields of the context map
 execute_command:
    Executes a command at the root of the project and returns its stdout
 add_readme_info:
    Add a message in context["resdir"]/README.txt
 pack_results:
    Compress results as a tar file and move it to context["resdir"]
 create_config_file_template:
    Generates a configuration template file

#### GIT functions / engine functions
 get_git_revisions_hash:
    Get the full hash
 get_git_branch_name:
    Specify a branch name
 compile_engine:
    Compile an engine from scratch
 update_engine:
    Pull from a specific commit

#### OS functions
 modify_python_path:
    Change the python path to set the one for the engine you would like to bench
 set_gpu_use:
    Set a specific env var to tell on which GPU(s) you would like to run
 get_compilation_flags:
    Get compilation flags used to compile _upstride.so
 setup_env:
    Pip install few modules that can missing to run benchs/tests
 print_generic_info:
    Print information about the system and the OS
 write_generic_info:
    Write generic information in a context["resdir"]/INFO.txt file
 print_error:
    Print error message using red color
 print_warning:
    Print warning message using yellow color
 print_info:
    Print information message using grey background
 print_dbg:
    Print debug message only when debug mode is enabled

### Run example
 python3 benchmarks.py # Just print info context and os
 # Run interactive mode that gonna ask to you to fill all information about what you want to run
 python3 benchmarks.py -i
 # Run benchmarks on the latest commit of the master branch, distclean and compile, then run all bench using the _upstride.so of the project
 python3 benchamrks.py --branch=master --update=True --gen_all_res --lib_path=/path/to/_upstride.so
 # Generate all results possible using default information we have
 python3 benchamrks.py -gar

"""
import functools    # static function
import nvgpu        # to get gpu info
import subprocess   # to run cmd, get printed log and check if process went fine or not
import platform     # to extract systel information
import os           # to run os.path.join, or os.system
import sys          # to manage PYTHONPATH
import argparse     # to parse input args
import configparser # to parse config files
import datetime     # datetime.datetime.now()
import platform     # platform and CPU info

# or any {'0', '1', '2'} # disable Tensorflow warning and message
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

context = {
    "project_name": "phoenix_tf", # project's name; /!\ Beware, here it represents the name of the root folder of the project.
                                  # If you put phoenix_tf in /opt/src in your docker, so the project_name should be "src"
    "device": "GPU",    # On which device you would like to compile/execture the engine
    "commit": "master", # Branch, tag or commit's hash
    "resdir": ".",      # Set path where you would like to push your results.
    "lib_path": [],     # path to the _upstride.so
    "engines": [],      # engines to compare
    "tensorflow": "",   # Just for consistency
    "py-upstride": "",  # Path to the python engine (i.e. upstride_python)
    "px-upstride": "",  # Python Path to the phoenix engine (i.e. phoenix_tf/src/python)
    "list_op": [],      # list of operations can contains also #epochs and batch_size as follow list_op["file"]={"batch_size"=bs, "epochs"=nb_epochs}
    "config_file": "",  # Instead of writing arguments you can provide a configuration file where all fields are already filled
    "mem_use_limit": 20,# GPU memory limit percent from where you consider it as usable
    "imagenet_architectures": [], # List all architecture to test with imagenet-baseline
    "orig_pythonpath": "",  # Save the PYTHONPATH env variable before we run this script
    ## ENV CONTEXT
    "DEBUG": True,      # Allow to print all debug messages
    "VERBOSE": 5        # Determine the verbose level; 0 completely silent, 1 only error messages, 2 add warning messages, 3 add info messages, 4 add debug messages
}


########################################
##         OS/GIT information         ##
########################################

@functools.lru_cache(maxsize=1)
def get_project_path():
    """Extract path to the <project name> root
    """
    path = os.path.abspath(os.getcwd())
    path = os.path.join(path.split(context["project_name"])[0], context["project_name"])
    return path


@functools.lru_cache(maxsize=1)
def get_lib_path():
    return context["lib_path"] if context["lib_path"] else os.path.join(get_project_path(), "build/libs")


@functools.lru_cache(maxsize=1)
def get_git_revisions_hash():
    """Return the current commit hash
    """
    return execute_command('git rev-parse HEAD')


@functools.lru_cache(maxsize=1)
def get_git_branch_name():
    """Return current branch name
    """
    branch_name = execute_command('git branch')
    return branch_name.split("* ", 1)[-1].split("\n", 1)[0]


@functools.lru_cache(maxsize=1)
def get_system_info():
    """Return following system information - current architecture and current operating system
    """
    return [platform.machine(), platform.platform()]


@functools.lru_cache(maxsize=1)
def get_available_gpus():
    """Return list of GPU indices that use less than mem_use_limit_persents% of its capacity or an empty list if no GPU is free
    """
    try:
        gpu_list_info = nvgpu.gpu_info()
        return [gpu["index"] for gpu in gpu_list_info if gpu["mem_used_percent"] < context["mem_use_limit"]]
    except:
        return []


def modify_python_path(engine="", clean=False):
    """Append sys.path in order to choose the right engine.
    """
    if clean:
        os.environ['PYTHONPATH'] = ""

    # Set PYTHONPATH env variable
    myenv = os.environ.copy()
    myenv['PYTHONPATH'] = context[engine]
    return myenv


def set_gpu_use(list_of_gpu_indices):
    """Export the list of GPU indices to use in the CUDA_VISIBLE_DEVICES env var
    """
    gpus = ",".join(list_of_gpu_indices)
    os.environ["CUDA_VISIBLE_DEVICES"] = gpus


# def docker_run(): # TODO in a future version

# def docker_create(): # TODO in a future version

########################################
##         Results Generation         ##
########################################


def collect_res_by_op(clean_all=True, clean=[]):
    """List operation to bench, run them and generate a log file that will be used by generate_graph_by_op later.
       Append all operations managed into the context list_op.
    Argument
        clean_all : remove associated log file before running benchmarks
        clean : List of log files to clean; file name is expected be the one of the run, not the log name
    """
    script_path = os.path.join(get_project_path(), 'scripts')
    print_info(f"Running all scripts *_bench.py in {script_path}")
    # find all files named *_bench.py and run them
    files = [f for f in os.listdir(script_path) if os.path.isfile(os.path.join(script_path, f))]
    if not files:
        print_warning(f"Beware, no file ending by \"_bench.py\" found in {script_path}")
    for f in files:
        if f.endswith("_bench.py"):
            file_path = os.path.join(script_path, f)
            op_name = f.replace('_bench.py', '')
            fname = os.path.splitext(f)[0]
            print_info(f"Run {f}")
            
            if not os.path.exists(os.path.join(context["resdir"], op_name)):
                os.makedirs(os.path.join(context["resdir"], op_name))

            logfile_withpath = os.path.join(context["resdir"], op_name, f'log_{fname}.log')
            # remove _bench.py to get only the op name from the file name.
            if  os.path.exists(logfile_withpath) and (clean_all or f in clean):
                os.system(f'rm {logfile_withpath}')

            # Set env for the phoenix engine 
            myenv = modify_python_path(engine="px-upstride")

            with open(logfile_withpath, 'a') as logfile:
                logfile.write("Execution Output : \n")
                print_dbg(f'python3 {file_path} --logdir={context["resdir"]}')
                logtxt = subprocess.Popen(['python3',
                                            file_path,
                                            f'--logdir={context["resdir"]}'],
                                        stdout=subprocess.PIPE,
                                        stderr=subprocess.PIPE,
                                        env=myenv)
                for line in logtxt.stdout:
                    logfile.write(line.decode('utf-8'))
                _, err = logtxt.communicate()
                if err: 
                    print_error(f'During the collection of data of {op_name}. See {logfile_withpath} for more information.')
                    logfile.write(err.decode('utf-8'))


            print_info(f"Add {op_name} to the list of operations.")
            context["list_op"].append(op_name)
            print_dbg(f'List of operation : {context["list_op"]}')


def generate_graph_by_op():
    """ Generate graph from results obtained previously from collect_res_by_op
    """
    script_path = os.path.join(get_project_path(), 'scripts')
    print_info(f"Generate results using the bench_visu.py scripts in {script_path}")
    # find all files names *_bench_op_visu.py and call them to generate associated graphs
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    if not files:
        print_warning(f"Beware, no file \"bench_visu.py\" found in {script_path}")
    for f in files:
        for op in context["list_op"]:
            if f.endswith(f'{op}_bench_visu.py'):
                print_info(f"Generate graph for {op} with  {script_path}/{f}")
                try:
                    print_dbg(f'python3 {script_path}/{f} --logdir={context["resdir"]} --op={op}')
                    subprocess.check_output(['python3',
                                            f'{script_path}/{f}',
                                            f'--logdir={context["resdir"]}',
                                            f'--op={op}'],
                                        cwd=script_path)
                except:
                    print_error(f'{op} failed. Please look previous message for more information.')


def generate_gpu_profile(script_path_with_name, where=get_project_path()):
    """ Generate gpu profiling using nsys or nvprof on an operation in paramter
    Command line example:
        $   nsys profile -o profetch.prof  --stats=true python3 tests/python_high_level_tests/train_simple_network_quaternions.py
        or
        $   nvprof -o profetch.prof python3 tests/python_high_level_tests/train_simple_network_quaternions.py  

    Not use yet be cause I'm not sure on what to run it, but will be very usefull soon.
    Should also work on CPU but we can find more appropriated tools (Intel VTune ?).
    """
    operation_name = script_path_with_name.split("\\", 1)[0]
    operation_path = script_path_with_name.split("\\", 1)[-1]
    if os.system('nsys') == 0:
        add_readme_info(f'Run nsys profile -o {os.path.join(context["resdir"], "prefetch.prof")} in {operation_path}')
        print_info(subprocess.check_output(['nsys',
                                            'profile',
                                            '-o',
                                            os.path.join(context["resdir"],"prefetch.prof"),
                                            'python3',
                                            operation_name],
                                        cwd=operation_path))
    elif os.system('nvprof'):
        add_readme_info(f'Run nvprof -o {os.path.join(context["resdir"], "prefetch.prof")} in {operation_path}')
        print_info(subprocess.check_output(['nvprof',
                                            '-o',
                                            os.path.join(context["resdir"],"prefetch.prof"),
                                            'python3',
                                            operation_name],
                                        cwd=operation_path))
    add_readme_info(f'To visualize results please open {context["resdir"]}/prefetch.prof with your NVIDIA Visual Profiler')


def generate_tensorboard_train_nn(epochs=30, batch_size=1024, list_of_files=[]):
    """Run all train_*.py files in tests/python_high_level_tests/
       these files should be able to execute tensorboard and move files in "--logdir=context["resdir"]/log_{filename}"
    """
    script_path = os.path.join(get_project_path(), 'tests', 'python_high_level_tests')
    print_info(f"[Small NN] Generating results using all training scripts in {script_path}")
    # Error key words that appear every time a error is raised with python
    # Far to be perfect but at least it's something ¯\_(ツ)_/¯
    errorKeywords = "Traceback (most recent call last)"
    # Decrease batch_size only when it crashed due to lack of memory
    errorMemoryLack = "out of memory"
    # find all files named train_*.py and fcall them to generate associated graphs
    # these files have been either requested in list_of_files or their name started with "train_"
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    if not files:
        print_warning(f"Beware, no file of type \"train_*.py\" found in {script_path}")

    for f in files:
        if list_of_files and f not in list_of_files:
            continue
        elif not f.startswith("train_"):
            continue

        # if the user define which file use to bench, we check if he also defined specific batch_size and number of epochs
        batch_size_r = list_of_files[f]["batch_size"] if f in list_of_files and list_of_files[f]["batch_size"] else batch_size
        epochs_r     = list_of_files[f]["epochs"]     if f in list_of_files and list_of_files[f]["epochs"]     else epochs

        print_info(f"Generation results from  {f}")
        name = os.path.splitext(f)[0]
        
        # Looking for a batch_size that fit to the GPU.
        # By default it takes 1024 as maximum to test and divide by two each time it fails until it reaches a batch_size equals to 1.
        look_for_batchsize = True
        success = False
        while look_for_batchsize:
            look_for_batchsize = True
            for engine in context["engines"]:               
                # Set names for logs
                logdir_name  = f'log_{engine}_{name}_{context["device"]}'
                logfile_name = f'log_{engine}_{name}_bs{batch_size_r}_{context["device"]}.log'
                logdir = os.path.join(context["resdir"], logdir_name)
                
                # Clean previous noisy results
                if os.path.exists(logdir):
                    subprocess.check_output(['rm', '-rf', f'{context["resdir"]}/{logdir_name}'])
                os.makedirs(logdir)

                # Set PYTHONPATH env variable
                myenv = modify_python_path(engine=engine)

                # Run cmd with the selected engine                    
                print_dbg(f'python3 {f} --logdir={logdir} --epochs={epochs} --batch_size={batch_size_r} 2>&1 {logdir}/{logfile_name}')
                with open(os.path.join(logdir, logfile_name), 'a') as logfile:
                    logfile.write(f"Run: python3 {f} --logdir={logdir} --epochs={epochs} --batch_size={batch_size_r}")
                    logfile.write("Execution Output : \n")
                    logtxt = subprocess.Popen(['python3', f,
                                            f'--upstride={0 if engine == "tensorflow" else 1}',
                                            f'--logdir={logdir}',
                                            f'--epochs={epochs_r}',
                                            f'--batch_size={batch_size_r}'],
                                            cwd=script_path,
                                            stdout=subprocess.PIPE,
                                            stderr=subprocess.PIPE,
                                            env=myenv)
                    while True:
                        line = logtxt.stdout.readline().decode('utf-8')
                        if not line: break
                        sys.stdout.write(line)  # print in standard output
                        logfile.write(line)
                    # Catch if an error was raised
                    _, err = logtxt.communicate()
                    if errorKeywords in err.decode('utf-8'):
                        print(err.decode('utf-8'))
                        logfile.write(err.decode('utf-8'))
                        print_warning(f"[Small NN] Batch_size of {batch_size_r} is not working for {f}.")
                        # If the error is "out of memory" let's try with a smaller batch_size
                        if batch_size_r > 1 and errorMemoryLack in err.decode('utf-8'):
                            print_warning("It seems related to a lack of memory. Let's try with a twice smaller batch_size.")
                            batch_size_r = int(batch_size_r/2)
                            break
                        else:
                            print_error("No \"batch_size\" works, so either your test is too greedy either the problem came from something else. Please check errors messages above.")
                            look_for_batchsize = False
                    else:
                        success = True
                        add_readme_info(f'[Small NN] Sucess on {f} with {engine}. \nTo visualize results please run: \ntensorboard --logdir {logdir}/{logdir_name}')
            
            look_for_batchsize = False if success else True


# TODO rename function according to the future new name of "imagenet-baselines" repo
def generate_imagenet_infer_results():
    """Download imagenet-baseline from bitbucket if it doesn't already exist in scripts/
       Then, if there is a config.yml file, use it to provide parameters. Otherwise, run fixed parameters
       This part of the script will be improved in the future: we will take more information in the context to provide here
    """
    # Clone imagnet-baseline
    script_path = os.path.join(get_project_path(), 'scripts')
    repository_path = os.path.join(script_path, 'imagenet-baselines')
    # Error key words that appear every time a error is raised with python
    # Far to be perfect but at least it's something ¯\_(ツ)_/¯
    errorKeywords = "Traceback (most recent call last)"
    # List of possible errors related to a lack of memory
    errorMemoryLack = ["out of memory", "Cannot allocate output tensor", "CUDNN_STATUS_ALLOC_FAILED"]
    if not os.path.exists(repository_path):
        subprocess.check_output(['git',
                                 'clone',
                                 'git@bitbucket.org:upstride/imagenet-baselines.git'],
                                cwd=script_path)
        subprocess.check_output(['git',
                                 'submodule',
                                 'update',
                                 '--init',
                                 '--recursive'],
                                cwd=f'{script_path}/imagenet-baselines')

    # FIXME remove "and False" condition TODO not ready, we need to define a location for this config file
    if os.path.exists(os.path.join(repository_path, 'config.yml')) and False:
        print("Using config.yml")
        print(subprocess.check_output(['python3',
                                       f'{repository_path}/inference_benchmark.py',
                                       f'--yaml_config={repository_path}/config.yml']))
    else:
        models = context["imagenet_architectures"]
        batch_size = 128
        docker_img = 'local'
        dtype = 2
        factor = 4
        for model in models:
            batch_size_r = batch_size
            if context["device"] == "CPU":
                print_error(f'Error: Sorry, {model} cannot be executed on {context["device"]}. Move to the next model.')
                continue

            # create path for results
            results_dir = os.path.join(context["resdir"], 'imagenet', model)
            if not os.path.exists(results_dir):
                os.makedirs(results_dir)

            # Call the inference_benchmark script
            # Parameters that can either be defined in a config file or by ourselves
            # The config file is currently searched here: scripts/imagenet-baseline/config.yml
            look_for_batchsize = True
            success = False
            while look_for_batchsize:
                fail = 0
                for engine in context["engines"]:
                    memory_error = False
                    engine_name = engine
                    if engine in ["py-upstride", "px-upstride"]:
                        engine_name = engine_name+f'_{dtype}-f{factor}'

                    # Clean PYTHONPATH and add path to the px-upstride
                    myenv = modify_python_path(engine=engine, clean=True)

                    # Run the Phoenix engine
                    print_dbg(f"python3 {repository_path}/inference_benchmark.py --batch_size {batch_size_r} --cpu 'False' --docker_images {docker_img} --models {model} --engines {engine_name} --output {os.path.join(results_dir, f'{engine}_res.txt')}")
                    with open(os.path.join(results_dir, f'log_inf_benchmark_{engine}_bs{batch_size_r}.log'), 'a') as logfile:
                        logfile.write("Execution Output : \n")
                        logtxt = subprocess.Popen(['python3',
                                                f'{repository_path}/inference_benchmark.py',
                                                '--batch_size', str(batch_size_r),
                                                '--cpu', 'True' if context["device"] == "CPU" else 'False',
                                                '--docker_images', docker_img,
                                                '--models', model,
                                                '--engines', engine_name,
                                                '--output', os.path.join(results_dir, f'{engine}_res.txt')],
                                                cwd=repository_path,
                                                stdout=subprocess.PIPE,
                                                stderr=subprocess.PIPE,
                                                env=myenv)
                        
                        while True:
                            line = logtxt.stdout.readline().decode('utf-8')
                            if not line: break
                            sys.stdout.write(line)  # print in standard output
                            logfile.write(line)
                        # Catch if an error was raised
                        _, err = logtxt.communicate()
                        if errorKeywords in err.decode('utf-8'):
                            print(err.decode('utf-8'))
                            logfile.write(err.decode('utf-8'))
                            print_error(f'[imagnet-baseline] Error when running {engine}')
                            print_warning(f"[imagnet-baseline] Batch_size of {batch_size_r} is not working for {model}.")
                            fail += 1
                            
                            if batch_size_r > 1:
                                for key in errorMemoryLack:
                                    if key in err.decode('utf-8'):
                                        print_warning("It seems related to a lack of memory. We will try with a batch_size twice smaller.")
                                        batch_size_r = int(batch_size_r/2)
                                        memory_error = True
                                if memory_error:
                                    break
                            else:
                                print_error("Error: No batch_size works, so either your model is too greedy either the problem came from something else.")
                                add_readme_info("If all run, runs out of memory, try to add the following lines in you code:\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n\ttf.config.experimental.set_memory_growth(gpu, True)\n")
                                look_for_batchsize = False
                                if batch_size_r == 1:
                                    success=True # not really true but to finish to look for a lower batch size
                                    break  # I don't know if we should stop everything or continue with the other egines
                        else:
                            success = True
                            add_readme_info(f'[Imagenet-Baseline] Sucess on {model} with {engine}. Results are in {os.path.join(results_dir, f"log_inf_benchmark_px_bs{batch_size_r}.log")} ')
                
                look_for_batchsize = False if success or fail == len(context["engines"]) else True


def generate_all_results(collect_op=True, generate_op=True, generate_nn=True, imagenet=True):
    print_info("****")
    print_info("Start generating results...")
    if collect_op:
        print_info("**")
        print_info("Collection data by operation...")
        collect_res_by_op()
    if generate_op:
        print_info("**")
        print_info("Generating graph by operation...")
        generate_graph_by_op()
    if generate_nn:
        print_info("**")
        print_info("Generating graph by training script...")
        generate_tensorboard_train_nn()
    if imagenet:
        print_info("**")
        print_info("Generating results from imagenet-baseline...")
        generate_imagenet_infer_results()
    print_info("... End generating results")
    print_info("****")


########################################
##               Engine               ##
########################################

def compile_engine(full_clean=True, use_gpu=True):
    """Compile engine from sources
    """
    print_info(f'Compile the engine from {context["commit"]}')
    if full_clean:
        print(execute_command('make distclean'))
    else:
        print(execute_command('make clean'))
    if use_gpu:
        assert context["Device"].find("GPU") != -1, "No GPU is available and 'use_gpu' is set to True."
    enable_gpus = 'ON' if use_gpu else 'OFF'
    print(execute_command(f'make GPU={enable_gpus}'))
    print(execute_command('make install'))


def update_engine():
    """ Pull engine source from specific commit
    """
    print_info(f'Update the engine from {context["commit"]}')
    print(execute_command('git checkout ' + context["commit"]))
    print(execute_command('git pull --recurse-submodules'))


@functools.lru_cache(maxsize=1)
def get_compilation_flags():
    """ Extract compilation flags from the elf.
        It exist several way to do it.
        Not optimal and force us to compile with gcc :/
        String is not well cut yet, it just to have the string; maybe one day we should improve this function
        TODO find a better way to extract this operation, try with readelf for example as follow (this line is not working perfectly)
    # logtxt = subprocess.Popen(['readelf','-wi', get_lib_path(), '| grep DW_AT_producer'], 
    #                             cwd=os.path.join(get_project_path(),'scripts'), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    """
    logtxt = subprocess.Popen(['strings', '-a', os.path.join(get_lib_path(), "_upstride.so")],
                              cwd=os.path.join(get_project_path(), 'scripts'), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    return [line.decode('utf-8') for line in logtxt.stdout if line.decode('utf-8').find("GNU GIMPLE") != -1]


########################################
##                Misc                ##
########################################

def execute_command(cmd):
    """ Executes a command at the root of the project and returns its stdout.
    Raises CalledProcessError exception if its return code is not zero.
    """
    return subprocess.check_output(cmd.split(' '), cwd=get_project_path()).strip().decode('ascii')


def print_error(str):
    if context["VERBOSE"] != 0:
        print("\033[0;31m [Error]", str, "\033[1;37;39m")  # print in red


def print_warning(str):
    if context["VERBOSE"] > 1:
        print("\033[0;33m [Warning]", str, "\033[1;37;39m")  # print in yellow


def print_info(str):
    if context["VERBOSE"] > 2:
        print("\033[0;37;40m [INFO]", str, "\033[0;37;39m")  # print in cyan


def print_dbg(str):
    if context["DEBUG"] or context["VERBOSE"] > 3:
        print("\033[0;36m [DBG]", str, "\033[0;37;39m")  # print in grey


def fill_context(config_file="", project_name="phoenix_tf", commit="master", resdir="", pxpath="", pypath="", lib_path="", verbose=1):
    # Save original PYTHONPATH env variable
    if 'PYTHONPATH' in os.environ:
        context["orig_pythonpath"] = os.environ['PYTHONPATH']
    if config_file == "":
        # Basic information
        context["project_name"] = project_name
        context["device"] = 'GPU' if get_available_gpus() else 'CPU'
        context["commit"] = commit
        # Fill engines en their path
        if pxpath and os.path.exists(pxpath):
            context["px-upstride"] = pxpath
            context["engines"].append("px-upstride")
            # information where to find libs
            #context["lib_path"]["px-upstride"] = lib_path if lib_path != "" else os.path.join(get_project_path(), "build/libs")
        if pypath and os.path.exists(pypath):
            context["py-upstride"] = pypath
            context["engines"].append("py-upstride")
            # information where to find libs
            #context["lib_path"]["py-upstride"] = lib_path if lib_path != "" else os.path.join(get_project_path(), "../upstride_python/upstride/type2/tf/keras/")
        context["engines"].append("tensorflow")
        context["tensorflow"] = ""
        context["lib_path"] = ""
        # Where to put results
        context["resdir"] = "/tmp/res-phoenix-" + get_git_revisions_hash()[:6] if resdir == "" else resdir
        # Should we clear all previous results by default ?
        if os.path.exists(context["resdir"]):
            os.system(f'rm -rf {context["resdir"]}')
        os.makedirs(context["resdir"])
        context["VERBOSE"] = verbose
        if not context["imagenet_architectures"]:
            context["imagenet_architectures"] = ['AlexNetNCHW','MobileNetV2NCHW','MobileNetV2Cifar10NCHW']
    else:
        # User assume that all fileds are correct
        if not os.path.exists(config_file):
            print("Error: ", config_file, " doesn't exist.")
            exit(0)
        # TODO THIS PART HAS NOT BEEN TESTED, SO VERIFY IT WORKS AND THAT WE HAVE ALL INFORMATION NEEDED
        config = configparser.ConfigParser()
        config.read(config_file)
        if 'project_name' in config:
            context["project_name"] = config['project_name']
        if 'device' in config:
            context["device"] = config['device']
            if context["device"] == "GPU":
                if 'mem_use_limit' in config:
                    context["mem_use_limit"] = config['mem_use_limit']
        if 'commit' in config:
            context["commit"] = config['commit']
        if 'px-upstride' in config:
            context["px-upstride"] = config['engine']
        if 'py-upstride' in config:
            context["py-upstride"] = config['engine']
        if 'resdir' in config:
            context["resdir"] = config['resdir']
            if os.path.exists(context["resdir"]):
                os.system(f'rm -rf {context["resdir"]}')
            os.makedirs(context["resdir"])
        if 'lib_path' in config:
            context["lib_path"] = config['lib_path']
        if 'list_op' in config:
            context["list_op"] = config['list_op']


def setup_env():
    """ Setup your pip environment to be able to run bench (it's mainly to avoid to crash because it only missing these install)
    """
    subprocess.check_output(['python3', '-m', 'pip', '--disable-pip-version-check', 'install', 'seaborn'])
    subprocess.check_output(['python3', '-m', 'pip', '--disable-pip-version-check', 'install', 'keras-tuner'])
    subprocess.check_output(['python3', '-m', 'pip', '--disable-pip-version-check', 'install', 'upstride_argparse'])


def print_generic_info():
    """ To protect your eyes from all ugly message and other prints
    """
    print("=======================")
    print("= GENERIC INFORMATION =")
    print("=======================")
    print("Project name: ", {context["project_name"]})
    print("From ", get_project_path())
    print("Result directory: ", context["resdir"])
    print("Engines path: ")
    print("Phoenix : ",context["px-upstride"])
    print("Python  : ",context["py-upstride"])
    print("-----------------------")
    print("Phoenix lib information: ")
    print("Branch: ", get_git_branch_name())
    print("Git current hash: ", get_git_revisions_hash())
    print("Libraries location: ", context["lib_path"])
    print("Compilation flags: ", get_compilation_flags())
    print("-----------------------")
    print("Sys info: \n", get_system_info())
    found_gpu = context["device"].find("GPU") != -1
    if found_gpu:
        print("Device: [GPUs] - ", get_available_gpus())
    else:
        print("Device: CPU")
    print("Proc: ", platform.processor())
    print("=======================")


def write_generic_info():
    """Write context anf system information in a specific file in the results dire. Such as, we can find how and where the run has been executed
    """
    if not os.path.exists(context["resdir"]):
        os.makedirs(context["resdir"])

    with open(os.path.join(context["resdir"], 'INFO.txt'), 'w') as info:
        info.writelines('===============================\n')
        info.writelines('==           CONTEXT         ==\n')
        info.writelines('===============================\n')
        info.writelines(f'Project name: {context["project_name"]}\n')
        info.writelines(f'Branch: {get_git_branch_name()}\n')
        info.writelines(f'Commit hash: {get_git_revisions_hash()}\n')
        info.writelines("System info: \n")
        info.writelines(f'Platform: {platform.system()}')
        info.write('\nMore info: ')
        info.writelines(["%s " % item for item in get_system_info()])
        info.write('\n')
        info.write("Device -")
        if context["device"] == "GPU":
            info.write(" GPUs:")
            info.writelines(f'[{",".join(get_available_gpus())}]')
        else:
            info.writelines(" CPU: ")
            info.writelines(platform.processor())
        info.write('\n')
        info.writelines(f'Compilation flags: {get_compilation_flags()}\n')
        info.writelines(f'Libs: {context["lib_path"]}')
        info.write('\n')
        info.writelines("Engines path: \n")
        info.writelines(f'  px-upstride path: {context["px-upstride"]}\n')
        info.writelines(f'  py-upstride path: {context["py-upstride"]}\n')
        info.writelines('===============================\n')
        info.write('\n')


def add_readme_info(message):
    with open(os.path.join(context["resdir"], 'README.txt'), 'w') as info:
        info.write(message)


def pack_results():
    print("pack results in ", context["resdir"])
    if not os.path.exists(context['resdir']):
        print_error("No results to pack.")
    else:
        try:
            date = str(datetime.datetime.now())[:10]
            execute_command(f'tar -zcvf benchmarks_phoenix_{get_git_revisions_hash()[:6]}_{date}.tar.gz {context["resdir"]}')
            execute_command(f'chmod 644 benchmarks_phoenix_{get_git_revisions_hash()[:6]}_{date}.tar.gz')
            execute_command(f'mv benchmarks_phoenix_{get_git_revisions_hash()[:6]}_{date}.tar.gz {context["resdir"]}')
            print_info(f'An archive with all results is available here: \n{context["resdir"]}/benchmarks_phoenix_{get_git_revisions_hash()[:6]}_{date}.tar.gz')
        except:
            print_error("Results haven't been packed. Please check error message above for more information.")


def create_config_file_template():
    config = configparser.ConfigParser()
    config["project_name"] = "phoenix_tf"
    config["device"] = "GPU"
    config["commit"] = "master"
    config["resdir"] = "res"
    config["lib_path"] = ""
    config["px-upstride"] = ""
    config["py-upstride"] = ""
    config["list_op"] = []
    config["config_file"] = ""
    config["mem_use_limit"] = 10

    with open('config_teplate.cfg', 'w') as configfile:
        config.write(configfile)


########################################
##                Main                ##
########################################


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--commit',         "-c",    type=str,  default="master",   help='git commit hash used to execute bench')
    parser.add_argument('--config-file',    "-cf",   type=str,  default="",         help='overight all other command except project name.')
    parser.add_argument('--gen_all_res',    "-gar",  action='store_true',           help='Run all bench')
    parser.add_argument('--interactive',    "-i",    action='store_true',           help='overight all other command except project name.')
    parser.add_argument('--lib_path',       "-lp",   type=str, default="",          help='Used to indicate where is _upstride.so')
    parser.add_argument('--project_name',   "-pn",   type=str, default="phoenix_tf",help='Default: phoenix_tf')
    parser.add_argument('--px_engine_path', "-pxep", type=str, default="",          help='Used to indicate where is located the Phoenix engine')
    parser.add_argument('--py_engine_path', "-pyep", type=str, default="",          help='Used to indicate where is located the python engine')
    parser.add_argument('--update',         "-u",    action='store_true',           help='Update and compile engine')
    parser.add_argument('--verbose',        "-v",    type=int, default=5,           help='Update and compile engine')
    args = parser.parse_args()

    # Either run following steps
    # Or ask to the user what he want to run
    # Only "general_all_res" can be ran if not in interactive mode
    if (not args.interactive):
        print_info = True
        update = False
        general_all_res = args.gen_all_res
        collect_op = False
        generate_op = False
        generate_nn = False
        imagenet = False
    else:
        txt = input("Print sys info? y/N ")
        print_info = True if (txt == "y") else False
        txt = input("Pull and compile? y/N ")
        if txt == "y":
            update = True
            txt = input("From which branch or commit? (Type a branch name, commit hash or nothing for master)")
            context["commit"] = txt if txt else "master"
        else:
            update = False
            context["commit"] = get_git_revisions_hash()

        txt = input("Run all benchmarks? y/N ")
        if txt == "y":
            general_all_res = True
            collect_op = False
            generate_op = False
            generate_nn = False
            imagenet = False
        else:
            general_all_res = False
            txt = input("Collect data by operation (scripts/*_bench.py)? y/N ")
            collect_op = True if (txt == "y") else False
            txt = input("Generate graph from previous collected data (by op)? y/N ")
            generate_op = True if (txt == "y") else False
            txt = input("Generate graph from small NN sample? y/N ")
            generate_nn = True if (txt == "y") else False
            txt = input("Run imagenet-baseline ? y/N ")
            imagenet = True if (txt == "y") else False
            if imagenet:
                txt = input("[imagenet-baseline] on AlexNet ? y/N ")
                if txt == 'y':
                    context["imagenet_architectures"].append('AlexNetNCHW')
                txt = input("[imagenet-baseline] on MobileNetV2Cifar10 ? y/N ")
                if txt == 'y':
                    context["imagenet_architectures"].append(
                        'MobileNetV2Cifar10NCHW')
                txt = input("[imagenet-baseline] on MobileNetV2 ? y/N ")
                if txt == 'y':
                    context["imagenet_architectures"].append('MobileNetV2NCHW')

    fill_context(project_name=args.project_name,
                 commit=args.commit,
                 pypath=args.py_engine_path if args.py_engine_path else os.path.join(get_project_path(), "../upstride_python"),  # TODO: this should not be default path but it's easier for me
                 pxpath=args.px_engine_path if args.px_engine_path else os.path.join(get_project_path(), "src/python"),
                 lib_path=args.lib_path,
                 verbose=args.verbose)

    set_gpu_use(get_available_gpus())
    setup_env()
    write_generic_info()

    if print_info:
        print_generic_info()

    if update:
        update_engine()
        compile_engine(full_clean=False)

    # Plot and generating res
    generate_all_results(collect_op=collect_op or general_all_res,
                         generate_op=generate_op or general_all_res,
                         generate_nn=generate_nn or general_all_res,
                         imagenet=imagenet       or general_all_res)
    # if results directory exists and contains results
    if os.path.exists(context["resdir"]) and len(os.listdir(context["resdir"])) > 1:
        pack_results()
    else:
        print_warning("No results found. You should check previous process messages for more information.")
    
    os.environ['PYTHONPATH'] = context["orig_pythonpath"]

if __name__ == "__main__":
    main()
