"""
#### Analysis Functions
# low level bench by operation or integrated in a very small NN
 collect_res_from_op/nn:
        collect data in a log file from all python script finishing by "_bench.py" (resp. "_bench_nn.op")
        these tests files should only contain one operation
        to test in the same way small NN, a second function is called to run all python script finishing by "_bench_nn.py"
 generate_graph_by_op/nn:
        Use results in log files generated by "collect_res_from_op" to generate graphs that are saved in a res folder.
        Under the hood, it calls all scripts finishing by "_bench_visu.py", which should correspond to one "<op>_bench.py".
        All _bench_visu.py should be implement using the same pattern and generate files in the same folder etc.
# High level bench
 generate_tensorboard_train_nn:
        Run all train scripts in tests/python_high_level_tests/; the name must start by "train_"
        All results are sent to {context["resdir"]}/log_<file_name>
 generate_tensorboard_infer_nn:
        Same as previously, but with inference tests; the name must start by "inf_"
        All results are sent to {context["resdir"]}/log_<file_name>
# High high level tests using specific architectures
 generate_imagenet_infer_results:
        Clone imagenet-baseline repository and run inference_benchmark.py,
        in order to get results in {context["resdir"]}
# To run all the functions above
 generate_results:
        Run all above functions

#### Misc Functions
 get_system_info:
    Return a list of information about the system (Architecture and OS); more will be added in the future
 get_available_gpus:
    Return the list of available gpus, empty list if no gpu were found.
 get_project_path:
    Get path to current project. By default look for the name "phoenix_tf"
 get_lib_path:
    Get the path to _upstride.so
 pretty_print:
    Protect your eyes from bleeding reading ugly messages
 fill_context:
    Fill all fields of the context map

#### GIT functions / engine functions
 get_git_revisions_hash:
    Get the full hash
 get_git_branch_name:
    Specify a branch name
 compile_engine:
    Compile an engine from scratch
 update_engine:
    Pull from a specific commit

#### OS functions
 modify_python_path:
    Change the python path to set the one for the engine you would like to bench
 set_gpu_use:
    Set a specific env var to tell on which GPU(s) you would like to run
 get_compilation_flags:
    Get compilation flags used to compile _upstride.so
 setup_env:
    Pip install few things that can missing to run benchs
 print_generic_info:
    Print information about the system and the OS
 write_generic_info:
    Write generic information in a file

### Run example
 python3 benchmarks.py # Just print info context and os
 # Run interactive mode that gonna ask to you to fill all information
 python3 benchmarks.py -i
 # Run benchmarks on the latest commit of the master branch, distclean and compile, then run all bench using the _upstride.so of the /path/to
 python3 benchamrks.py --branch=master --update=True --gen_all_res=True --lib_path=/path/to/_upstride.so

"""
import functools  # static function
import nvgpu  # to get gpu info
import subprocess  # to run cmd, get printed log and check if process went fine or not
import platform  # to extract systel information
import os  # to run os.path.join, or os.system
import sys  # to manage PYTHONPATH
import argparse  # to parse input args
# or any {'0', '1', '2'} # disable Tensorflow warning and message
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

context = {
    "project_name": "phoenix_tf",  # project's name; /!\ Beware, here it represents the name of the root folder of the project. 
    # If you put phoenix_tf in /opt/src in your docker, so the project_name should be "src"
    "device": "GPU", # to decide on which device you would like to compile/execture the engine
    "commit": "master", # Branch, tag or commit's hash
    "resdir": "res", # Set path where you would like to push your results. /!\ Beware: not well managed (for the moment) 
                     # for collect_res_from_op and generate_graph_by_op
    "lib_path": "",  # path to the _upstride.so
    # which engine to use; /!\ Beware, for now it is only use with the imagenet-baseline bench
    "engine": "px-engine",
    "py-engine": "", # Path to the python engine (i.e. upstride_python)
    "px-engine": ""  # Python Path to the phoenix engine (i.e. phoenix_tf/src/python)
}

########################################
##         OS/GIT information         ##
########################################

@functools.lru_cache(maxsize=1)
def get_project_path():
    """Extract path to the <project name> root
    """
    path = os.path.abspath(os.getcwd())
    path = os.path.join(path.split(context["project_name"])[0], context["project_name"])
    return path


@functools.lru_cache(maxsize=1)
def get_lib_path():
    return context["lib_path"] if context["lib_path"] else os.path.join(get_project_path(), "build/libs")


@functools.lru_cache(maxsize=1)
def get_git_revisions_hash():
    """Return the current commit hash
    """
    return execute_command('git rev-parse HEAD')


@functools.lru_cache(maxsize=1)
def get_git_branch_name():
    """Return current branch name
    """
    branch_name = execute_command('git branch')
    return branch_name.split("* ", 1)[-1].split("\n", 1)[0]


@functools.lru_cache(maxsize=1)
def get_system_info():
    """Return following system information - current architecture and current operating system
    """
    return [platform.machine(), platform.platform()]


@functools.lru_cache(maxsize=1)
def get_available_gpus(mem_use_limit_percents=10):
    """Return list of GPU indices that use less than mem_use_limit_persents% of its capacity or an empty list if no GPU is free
    """
    try:
        gpu_list_info = nvgpu.gpu_info()
        return [gpu["index"] for gpu in gpu_list_info if gpu["mem_used_percent"] < mem_use_limit_percents]
    except:
        return []


def modify_python_path(engine, clean=False):
    """Append sys.path in order to choose the right engine.
    """
    if clean:
        sys.path.remove(context["py-engine"])
        sys.path.remove(context["px-engine"])

    if engine.find("px-upstride") != -1:
        if context["px-engine"]:
            sys.path.append(context["px-engine"])
        else:
            sys.path.append(os.path.join(get_project_path(), 'build'))
            sys.path.append(os.path.join(get_project_path(), 'src/python'))
    # If Python engine
    elif engine.find("py-upstride") != -1:
        if context["py-engine"]:
            sys.path.append(context["py-engine"])
        else:
            sys.path.append(os.path.join(get_project_path()))
        # phoenix_tf/build: src/python # FIXME


def set_gpu_use(list_of_gpu_indices):
    """Export the list of GPU indices to use in the CUDA_VISIBLE_DEVICES env var
    """
    gpus = ",".join(list_of_gpu_indices)
    os.system(f"export CUDA_VISIBLE_DEVICES={gpus}")

# def docker_run(): # TODO in a future version

# def docker_create(): # TODO in a future version

########################################
##         Results Generation         ##
########################################


def collect_res_from_op(clean_all=True, clean=[]):
    """List operation to bench, run them and generate a log file that will be used by generate_graph_by_op later
    Argument
        clean_all : remove associated log file before running benchmarks
        clean : List of log files to clean; file name is expected be the one of the run, not the log name
    Return
        list of processed files
    """
    op_list = []
    script_path = os.path.join(get_project_path(), 'scripts')
    print(f"Running all scripts *_bench.py in {script_path}")
    # find all files named *_bench.py and run them
    files = [f for f in os.listdir(script_path) if os.path.isfile(os.path.join(script_path, f))]
    for f in files:
        if f.endswith("_bench.py"):
            if clean_all or clean.find(f):
                print("Clean log")
                os.system(f'rm {script_path}/log_{f}.log')

            file_path = os.path.join(script_path, f)
            print("Run ", file_path)

            with open(os.path.join(script_path, f'log_{f}.log'), 'a') as log:
                log.write("Execution Output : \n")
                logtxt = subprocess.Popen(['python3', file_path], cwd=script_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
                for line in logtxt.stdout:
                    log.write(line.decode('utf-8'))
                op_list.append(f)
    return op_list


def collect_res_from_nn(clean_all=True, clean=[]):
    """List operations to bench, run them and generate a log file that will be used by generate_graph_by_nn later
    Argument
        clean_all : remove associated log file before to run benchmarks
        clean : List of log file to clean; file name must be the one of the run not the log name
    Return
        list of processed files
    """
    nn_list = []
    script_path = os.path.join(get_project_path(), 'scripts')
    print(f"Running all scripts *_bench_nn.py in {script_path}")
    # find all files named *_bench_nn.py and run them
    files = [f for f in os.listdir(f'{get_project_path()}/scripts/') if os.path.isfile(f'{get_project_path()}/scripts/{f}')]
    for f in files:
        if(f.endswith("_bench_nn.py")):
            if clean_all or clean.find(f):
                os.system(f'rm {script_path}/log_{f}.log')
            file_path = os.path.join(script_path, f)
            print("Run ", file_path)
            with open(os.path.join(script_path, f'log_{f}.log'), 'a') as log:
                log.write("Execution Output : \n")
                logtxt = subprocess.Popen(['python3', file_path], cwd=script_path, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
                for line in logtxt.stdout:
                    log.write(line.decode('utf-8'))
                nn_list.append(f)
    return nn_list


def collect_res(clean_all=True, clean=[]):
    collect_res_from_op(clean_all, clean)
    collect_res_from_nn(clean_all, clean)


def generate_graph_by_op():
    """ Generate graph from results obtained previously from collect_res_from_op
    """
    script_path = os.path.join(get_project_path(), 'scripts')
    print("Generate results using all scripts *_bench_visu.py in ", script_path)
    # find all files names *_bench_op_visu.py and call them to generate associated graphs
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    for f in files:
        if f.endswith("_bench_visu.py"):
            print(f'Generating graph from {script_path}/{f}')
            subprocess.check_output(['python3', f'{script_path}/{f}'], cwd=script_path)


def generate_graph_by_nn():
    """ Generate graph from results obtained previously from collect_res_from_nn
    """
    script_path = os.path.join(get_project_path(), 'scripts')
    print("Generate results using all scripts *_bench_nn_visu.py in ", script_path)
    # find all files named *_bench_nn_visu.py and call them to generate associated graphs
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    for f in files:
        if f.endswith("_bench_nn_visu.py"):
            print(f'Generating graph from {script_path}/{f}')
            subprocess.check_output(['python3', f'{script_path}/{f}'], cwd=script_path)


def generate_gpu_profile(operation_path_with_name):
    if context["device"] == "CPU":
        return
    operation_name = operation_path_with_name.split("\\", 1)[0]
    operation_path = operation_path_with_name.split("\\", 1)[-1]
    if os.system('nsys') == 0:
        print(subprocess.check_output(['nsys', 'profile', '-o', os.path.join(
            context["resdir"], "prefetch.prof"), 'python3', operation_name], cwd=operation_path))
    elif os.system('nvprof'):
        print(subprocess.check_output(['nvprof', '-o', os.path.join(
            context["resdir"], "prefetch.prof"), 'python3', operation_name], cwd=operation_path))
        # Command line example:
        #   nsys profile -o profetch.prof  --stats=true python3 tests/python_high_level_tests/train_simple_network_quaternions.py


def generate_tensorboard_train_nn(epochs=30, batch_size=1000):
    """Run all train_*.py files in tests/python_high_level_tests/
       these files should be able to execute tensorboard and move files in "--logdir=context["resdir"]/log_{filename}"
    """
    script_path = os.path.join(get_project_path(), 'tests', 'python_high_level_tests')
    print("[Training] Generate results using all training scripts in ", script_path)
    # find all files named train_*.py and fcall them to generate associated graphs
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    for f in files:
        if f.startswith("train_"):
            print("Generation results from ", f)
            name = os.path.splitext(f)[0]
            subprocess.check_output(['rm', '-rf', f'{context["resdir"]}/log_{name}'])
            subprocess.check_output(['python3', f, f'--logdir={context["resdir"]}/log_{name}',
                                        f'--epochs={epochs}', f'--batch_size={batch_size}'], cwd=script_path)


def generate_tensorboard_infer_nn(epochs=30, batch_size=1000):
    """Run all inf_*.py files in tests/python_high_level_tests/
         these files should be able to execute tensorboard and move files in "--logdir=context["resdir"]/log_{filename}"
    """
    script_path = os.path.join(get_project_path(), 'tests', 'python_high_level_tests')
    print("[Inference] Generate results using all inference scripts in ",
          get_project_path(), "/tests/python_high_level_tests/")
    # find all files named inf_*.py and fcall them to generate associated graphs
    files = [f for f in os.listdir(script_path) if os.path.isfile(f'{script_path}/{f}')]
    for f in files:
        if f.startswith("inf_"):
            print("Generation results from ", f)
            name = os.path.splitext(f)[0]
            subprocess.check_output(['rm', '-rf', f'{context["resdir"]}/log_{name}'])
            subprocess.check_output(['python3', f, f'--logdir={context["resdir"]}/log_{name}',
                                        f'--epochs={epochs}', f'--batch_size={batch_size}'])


def generate_imagenet_infer_results(): # TODO rename function according to the future new name of "imagenet-baselines" repo
    """Download imagenet-baseline from bitbucket if it doesn't already exist in scripts/
       Then, if there is a config.yml file, use it to provide parameters. Otherwise, run fixed parameters
       This part of the script will be improved in the future: we will take more information in the context to provide here
    """
    # TODO for a future version, create an argument for choosing among models
    # Clone imagnet-baseline
    script_path = os.path.join(get_project_path(), 'scripts')
    repository_path = os.path.join(script_path, 'imagenet-baselines')
    if not os.path.exists(repository_path):
        subprocess.check_output(['git', 'clone', 'git@bitbucket.org:upstride/imagenet-baselines.git'], cwd=script_path)
    # call inference_benchmark.py
    # with right parameters that can be defined in a config file or by ourselves
    # this config file is searched here: scripts/imagenet-baseline/config.yml
    print("For Tensorflow engine:")
    if os.path.exists(os.path.join(repository_path, 'config.yml')) and False: # FIXME remove "and False" condition
        print("Using config.yml")
        print(subprocess.check_output(['python3',
                                       f'{repository_path}/inference_benchmark.py',
                                       f'--yaml_config={repository_path}/config.yml']))
    else:
        print(f'[bench] CMD RUN =  python3',
              f'{repository_path}/inference_benchmark.py',
              '--batch_size', '1000',
              # '--cpu', 'True',
              # '--docker_images', 'tensorflow/tensorflow:2.3.0-gpu',
              '--models', 'AlexNetNCHW',
              '--engines', 'tensorflow', 'py-upstride_2-f4', 'px-upstride_2-f4')
        # create path for results
        results_dir = os.path.join(context["resdir"], 'imagenet', 'Alexnet')
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
         # Run imagenet-baseline benchmark script
        print(subprocess.check_output(['python3',
                                       f'{repository_path}/inference_benchmark.py',
                                       '--batch_size', '1000',
                                       # '--cpu','True',
                                       # '--docker_images','tensorflow/tensorflow:2.3.0-gpu' if context["device"] == "GPU" else 'local',
                                       '--docker_images', 'local',
                                       '--models', 'AlexNetNCHW',
                                       '--engines', 'tensorflow',
                                       '--output', os.path.join(results_dir, 'tf_res.txt')],
                                      cwd=repository_path))
        #####
        ###
        ##
        # This part has not been well tested so it may crash, it this case try t run first the commande print before and try to adapt the "--engine <engine>" by replace <engine> by either px-engine_2-f4 or py-engine_2-f4
        # (Note: the engine name doesn't really matter. The main point is to call one name containing "upstride_<type>-factorN", to use the right engine only the PYTHONPATH is used, ask to @Sebastien for more information)
        ##
        ###
        ####
        # if the "py-engine" field isn't empty, it means that we would like to use it
        if context["py-engine"] != "":
            # change PYTHONPATH
            modify_python_path("py-upstride", clean=True)
            print(subprocess.check_output(['python3',
                                           f'{repository_path}/inference_benchmark.py',
                                           '--batch_size', '1000',
                                           # '--cpu','True',
                                           # '--docker_images','tensorflow/tensorflow:2.3.0-gpu' if context["device"] == "GPU" else 'local',
                                           '--docker_images', 'local',
                                           '--models', 'AlexNetNCHW',
                                           '--engines', 'py-upstride_2-f4'
                                           '--output', os.path.join(results_dir, 'pyupstride_res.txt')],
                                          cwd=repository_path))
        # if the "px-engine" field isn't empty, it means that we would like to use it
        if context["px-engine"] != "":
            # change PYTHONPATH
            modify_python_path("px-upstride", clean=True)
            print(subprocess.check_output(['python3',
                                           f'{repository_path}/inference_benchmark.py',
                                           '--batch_size', '1000',
                                           # '--cpu','True',
                                           # '--docker_images','tensorflow/tensorflow:2.3.0-gpu' if context["device"] == "GPU" else 'local',
                                           '--docker_images', 'local',
                                           '--models', 'AlexNetNCHW',
                                           '--engines', 'px-upstride_2-f4'
                                           '--output', os.path.join(results_dir, 'pxupstride_res.txt')],
                                          cwd=repository_path))
    # get results


def generate_all_results(collect=True):
    print("Start generating results...")
    if collect:
        print("Collection data by operation...")
        collect_res_from_op()
        collect_res_from_nn()
    print("Generating graph by operation...")
    generate_graph_by_op()
    generate_graph_by_nn()
    print("Generating graph by training script...")
    generate_tensorboard_train_nn()
    print("Generating graph by inference script...")
    generate_tensorboard_infer_nn()
    print("Generating results from imagenet-baseline...")
    generate_imagenet_infer_results()
    print("... End generating results")

########################################
##               Engine               ##
########################################
    
def compile_engine(full_clean=True, use_gpu=True):
    """Compile engine from sources
    """
    print("Compile the engine from ", context["commit"])
    if full_clean:
        print(execute_command('make distclean'))
    else:
        print(execute_command('make clean'))
    if use_gpu:
        assert context["Device"].find("GPU") != -1, "No GPU is available and 'use_gpu' is set to True."
    with_cuda = 'ON' if use_gpu else 'OFF'
    print(execute_command(f'make WITH_CUDNN={with_cuda}'))

def update_engine():
    """ Pull engine source from specific commit
    """
    print("Update the engine from ", context["commit"])
    print(execute_command('git checkout ' + context["commit"]))
    print(execute_command('git pull --recurse-submodules'))

@functools.lru_cache(maxsize=1)
def get_compilation_flags():
    """ Extract compilation flags from the elf.
        It exist several way to do it.
        Not optimal and force us to compile with gcc :/
        String is not well cut yet, it just to have the string; maybe one day we should improve this function
    """
    logtxt = subprocess.Popen(['strings', '-a', os.path.join(get_lib_path(), "_upstride.so")],
                              cwd=os.path.join(get_project_path(), 'scripts'), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    # logtxt = subprocess.Popen(['readelf','-wi', get_lib_path(), '| grep DW_AT_producer'], 
    #                             cwd=os.path.join(get_project_path(),'scripts'), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    # FIXME commented line
    return [line.decode('utf-8') for line in logtxt.stdout if line.decode('utf-8').find("GNU GIMPLE") != -1]

########################################
##                Misc                ##
########################################

def execute_command(cmd):
    """ Executes a command in the current folder and returns its stdout.
    Raises CalledProcessError exception if its return code is not zero.
    """
    return subprocess.check_output(cmd.split(' '), cwd=get_project_path()).strip().decode('ascii')
    

def fill_context(project_name="phoenix_tf", commit="master", engine="px-engine", lib_path=""):
    context["project_name"] = project_name
    context["device"] = 'GPU' if get_available_gpus() else 'CPU'
    context["commit"] = commit
    context["engine"] = engine
    context["resdir"] = "/tmp/res-" + engine + "-commit"
    if os.path.exists(context["resdir"]):
        os.system(f'rm -rf {context["resdir"]}')
    context["lib_path"] = lib_path if lib_path != "" else os.path.join(get_project_path(), "build/libs")


def setup_env():
    """ Setup your pip environment to be able to run bench (it's mainly to avoid to crash because it only missing these install)
    """
    subprocess.check_output(['python3', '-m', 'pip', '--disable-pip-version-check', 'install', 'seaborn'])
    subprocess.check_output(['python3', '-m', 'pip', '--disable-pip-version-check', 'install', 'keras-tuner'])


def print_generic_info():
    print("=======================")
    print("git current hash: ", get_git_revisions_hash())
    print("Branch: ", get_git_branch_name())
    print("Sys info: ", get_system_info())
    found_gpu = context["device"].find("GPU") != -1
    if found_gpu:
        print("Device(s): ", get_available_gpus())
    else:
        print("Device: CPU")
    print("Compilation flags: ", get_compilation_flags())
    print("engine: ", context["engine"])
    print("lib_path: ", context["lib_path"])
    print("results path: ", context["resdir"])
    if found_gpu:
        print("List of available GPUS: ")
        os.system("echo $CUDA_VISIBLE_DEVICE")
    print("-----------------------")


def write_generic_info():
    """Write context anf system information in a specific file in the results dire. Such as, we can find how and where the run has been executed
    """
    if not os.path.exists(context["resdir"]):
        os.makedirs(context["resdir"])
    with open(os.path.join(context["resdir"], 'INFO.txt'), 'w') as info:
        info.write("Commit hash: ".join(get_git_revisions_hash()))
        info.write("Branch: ".join(get_git_branch_name()))
        info.writelines(["System info: %s" % item for item in get_system_info()])
        info.writelines("GPUs: %s" % item for item in get_available_gpus())
        info.write("Compilation flags:".join(get_compilation_flags()))
        info.write("engine: ".join(context["engine"]))
        info.writelines("engine compiled with: %s" %
                        item for item in context["lib_path"])


def pretty_print():
    """ To protect your eyes from all ugly message and other prints
    """
    print_generic_info()
    print("From ", get_project_path())
    print("Project name: ", context["project_name"])
    print("Device:", context["device"])
    print("Git commit: ", context["commit"])
    print("Result directory: ", context["resdir"])
    print("Libraries location: ", context["lib_path"])
    print("Which engine: ", context["engine"])
    print("Engine path: ", context[context["engine"]])
    print("=======================")

########################################
##                Main                ##
########################################


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--project_name', "-pn",     type=str, default="phoenix_tf", help='Default: phoenix_tf')
    parser.add_argument('--commit', "-c",            type=str, default="master",     help='git commit hash used to execute bench')
    parser.add_argument('--update', "-u",            type=str, default=True,         help='Update and compile engine')
    parser.add_argument('--engine', "-e",            type=str, default="px-engine",  help='Possible value: \"px-engine\"(default), \"py-engine\"')
    parser.add_argument('--gen_all_res', "-gar",     type=bool,default=False,        help='Run all bench')
    parser.add_argument('--lib_path', "-lp",         type=str, default="",           help='Used to indicate where is _upstride.so')
    parser.add_argument('--py_engine_path', "-pyep", type=str, default="",           help='Used to indicate where is located the python engine')
    parser.add_argument('--px_engine_path', "-pxep", type=str, default="",           help='Used to indicate where is located the Phoenix engine')
    parser.add_argument('--interactive', "-i", action='store_true',                  help='overight all other command except project name.')
    args = parser.parse_args()

    # Either run following steps
    # Or ask to the user what he want to run
    if (not args.interactive):
        print_info = True
        update = False
        general_all_res = args.gen_all_res
        collect_res_op = False
        collect_res_nn = False
        generate_res_op = False
        generate_res_nn = False
        imagenet = False
    else:
        txt = input("Print sys info? y/n ")
        print_info = True if (txt == "y") else False
        txt = input("Pull and compile? y/n ")
        if txt == "y":
            update = True
            txt = input("From which branch or commit? (Type a branch name, commit hash or nothing for master)")
            context["commit"] = txt if txt else "master"
        else:
            update = False
            context["commit"] = get_git_revisions_hash()

        txt = input("Run all benchmarks? y/n ")
        if txt == "y":
            general_all_res = True
            collect_res_op = False
            collect_res_nn = False
            generate_res_op = False
            generate_res_nn = False
            imagenet = False
        else:
            txt = input("Collect data by operation (scripts/*_bench.py)? y/n ")
            collect_res_op = True if (txt == "y") else False
            txt = input("Collect data by neural network script (scripts/*_bench_nn.py)? y/n ")
            collect_res_nn = True if (txt == "y") else False
            txt = input("Generate graph from previous collected data (by op)? y/n ")
            generate_res_op = True if (txt == "y") else False
            txt = input("Generate graph from previous collected data (by nn)? y/n ")
            generate_res_nn = True if (txt == "y") else False
            txt = input("Run imagenet-baseline AlexNet? y/n ")
            imagenet = True if (txt == "y") else False

    fill_context(project_name=args.project_name,
                 commit=args.commit,
                 engine=args.engine,
                 lib_path=args.lib_path)

    set_gpu_use(get_available_gpus())
    setup_env()

    context["py-engine"] = args.py_engine_path if args.py_engine_path else os.path.join(get_project_path(), "../upstride_python")
    context["px-engine"] = args.px_engine_path if args.px_engine_path else os.path.join(get_project_path(), "src/python")

    if print_info:
        pretty_print()

    if update:
        update_engine()
        compile_engine(full_clean=False)

    # Data collection
    if collect_res_op:
        collect_res_from_op()
    if collect_res_nn:
        collect_res_from_nn()

    # Plot and generating res
    if general_all_res:
        generate_all_results(collect=True)
    else:
        if generate_res_op:
            generate_graph_by_op()
        if generate_res_nn:
            generate_tensorboard_train_nn()
            generate_tensorboard_infer_nn()
        if imagenet:
            generate_imagenet_infer_results()


if __name__ == "__main__":
    main()
